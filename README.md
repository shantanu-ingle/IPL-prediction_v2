# IPL Prediction System

## Overview

The IPL Prediction System is a web application that predicts outcomes of Indian Premier League (IPL) cricket matches using machine learning models and provides detailed explanations via a large language model (LLM). The system includes:

- **Frontend**: A React.js interface (`ipl-frontend`) for entering match details, fetching predictions, and interacting with a chatbot.
- **Backend**: A Django REST API (`ipl_prediction_new`) that uses PyTorch neural networks to predict match winners, team runs, player runs, and bowler stats, with explanations generated by Mistral (via Ollama).


## Features

- **Match Predictions**: Predict the match winner, team runs, and player runs with confidence intervals.
- **Explanations**: Detailed explanations for predictions generated by Mistral LLM.
- **Frontend Interface**: A React.js UI with Tailwind CSS styling, featuring a form for match details, prediction display, and a chatbot.
- **Chatbot**: Interactive chatbot that answers questions like "Who will win?" and "How many runs for [team]?".
- **Visualizations**: Player trend plot showing performance over the last 3 matches.


## Prerequisites

- **Node.js**: v16.x LTS or v20.x LTS (recommended for `create-react-app` compatibility).
- **Python**: v3.8+ (for Django backend).
- **Conda**: For managing Python environments (optional).
- **Ollama**: For running the Mistral LLM locally.

## Installation

### 1. Clone the Repository
bash
git clone https://github.com/<your-username>/<your-repo>.git
cd ipl-frontend

### 2. Set Up the Backend (ipl_prediction)
Navigate to the Backend Directory:
bash
cd ipl_prediction

Create a Virtual Environment (Recommended):
Using Conda:
bash
conda create -n image_env python=3.8
conda activate image_env

Or using venv:
bash
python -m venv venv
.\venv\Scripts\activate

Install Python Dependencies:
django
djangorestframework
pandas
numpy
torch
scikit-learn
ollama
matplotlib
django-cors-headers

Set Up Ollama:
Install Ollama (follow instructions at ollama.ai).

Pull the Mistral model:
bash
ollama pull mistral

Start the Ollama server in a separate terminal:
bash
ollama serve

Run Migrations:
bash
python manage.py makemigrations
python manage.py migrate

Start the Django Server:
bash
python manage.py runserver
The backend should be running at http://localhost:8000.

### 3. Set Up the Frontend (ipl-frontend)

Install dependencies:
bash
npm install

Start the React App:
bash
npm start
The frontend should be running at http://localhost:3000.


![Screenshot 2025-05-06 181534](https://github.com/user-attachments/assets/2964e53e-1232-4b7e-914b-deb7a1016433)
![Screenshot 2025-05-06 181551](https://github.com/user-attachments/assets/312b9d80-833c-4a66-ae57-353679fd32e2)
![Screenshot 2025-05-06 181610](https://github.com/user-attachments/assets/fc6e0f8d-9d90-4eb6-a2d2-c925a33eb53d)
![Screenshot 2025-05-06 181622](https://github.com/user-attachments/assets/a2f5f395-5333-455a-840a-baf8812fcbe7)
![Screenshot 2025-05-06 181633](https://github.com/user-attachments/assets/bee59e79-fa22-4b6a-984e-58b1d364d6cd)




